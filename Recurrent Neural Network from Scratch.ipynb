{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recurrent Neural Network - Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward propagation in basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFNRtAP_ELFw"
   },
   "source": [
    "### Dimensions of input $x$\n",
    "\n",
    "#### Input with $n_x$ number of units\n",
    "* For a single timestep of a single input example, $x^{(i) \\langle t \\rangle }$ is a one-dimensional input vector.\n",
    "* Using language as an example, a language with a 5000 word vocabulary could be one-hot encoded into a vector that has 5000 units.  So $x^{(i)\\langle t \\rangle}$ would have the shape (5000,).  \n",
    "* We'll use the notation $n_x$ to denote the number of units in a single timestep of a single training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnYGy4L-ELFx"
   },
   "source": [
    "#### Time steps of size $T_{x}$\n",
    "* A recurrent neural network has multiple time steps, which we'll index with $t$.\n",
    "* In the lessons, we saw a single training example $x^{(i)}$ consist of multiple time steps $T_x$.  For example, if there are 10 time steps, $T_{x} = 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Azzhk7jCELFx"
   },
   "source": [
    "#### Batches of size $m$\n",
    "* Let's say we have mini-batches, each with 20 training examples.  \n",
    "* To benefit from vectorization, we'll stack 20 columns of $x^{(i)}$ examples.\n",
    "* For example, this tensor has the shape (5000,20,10). \n",
    "* We'll use $m$ to denote the number of training examples.  \n",
    "* So the shape of a mini-batch is $(n_x,m,T_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNR7VozOELFy"
   },
   "source": [
    "#### 3D Tensor of shape $(n_{x},m,T_{x})$\n",
    "* The 3-dimensional tensor $x$ of shape $(n_x,m,T_x)$ represents the input $x$ that is fed into the RNN.\n",
    "\n",
    "#### Taking a 2D slice for each time step: $x^{\\langle t \\rangle}$\n",
    "* At each time step, we'll use a mini-batches of training examples (not just a single example).\n",
    "* So, for each time step $t$, we'll use a 2D slice of shape $(n_x,m)$.\n",
    "* We're referring to this 2D slice as $x^{\\langle t \\rangle}$.  The variable name in the code is `xt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOzhhTj4ELFy"
   },
   "source": [
    "### Definition of hidden state $a$\n",
    "\n",
    "* The activation $a^{\\langle t \\rangle}$ that is passed to the RNN from one time step to another is called a \"hidden state.\"\n",
    "\n",
    "### Dimensions of hidden state $a$\n",
    "\n",
    "* Similar to the input tensor $x$, the hidden state for a single training example is a vector of length $n_{a}$.\n",
    "* If we include a mini-batch of $m$ training examples, the shape of a mini-batch is $(n_{a},m)$.\n",
    "* When we include the time step dimension, the shape of the hidden state is $(n_{a}, m, T_x)$\n",
    "* We will loop through the time steps with index $t$, and work with a 2D slice of the 3D tensor.  \n",
    "* We'll refer to this 2D slice as $a^{\\langle t \\rangle}$. \n",
    "* In the code, the variable names we use are either `a_prev` or `a_next`, depending on the function that's being implemented.\n",
    "* The shape of this 2D slice is $(n_{a}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67vYjIRTELFz"
   },
   "source": [
    "### Dimensions of prediction $\\hat{y}$\n",
    "* Similar to the inputs and hidden states, $\\hat{y}$ is a 3D tensor of shape $(n_{y}, m, T_{y})$.\n",
    "    * $n_{y}$: number of units in the vector representing the prediction.\n",
    "    * $m$: number of examples in a mini-batch.\n",
    "    * $T_{y}$: number of time steps in the prediction.\n",
    "* For a single time step $t$, a 2D slice $\\hat{y}^{\\langle t \\rangle}$ has shape $(n_{y}, m)$.\n",
    "* In the code, the variable names are:\n",
    "    - `y_pred`: $\\hat{y}$ \n",
    "    - `yt_pred`: $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZrlQ4X8ELFz"
   },
   "source": [
    "Here's how you can implement an RNN: \n",
    "\n",
    "**Steps**:\n",
    "1. Implement the calculations needed for one time-step of the RNN.\n",
    "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oXWAKeTELF0"
   },
   "source": [
    "## 1.1 - RNN cell\n",
    "\n",
    "A recurrent neural network can be seen as the repeated use of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"images/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $\\hat{y}^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "#### rnn cell versus rnn_cell_forward\n",
    "* Note that an RNN cell outputs the hidden state $a^{\\langle t \\rangle}$.  \n",
    "    * The rnn cell is shown in the figure as the inner box which has solid lines.  \n",
    "* The function that we will implement, `rnn_cell_forward`, also calculates the prediction $\\hat{y}^{\\langle t \\rangle}$\n",
    "    * The rnn_cell_forward is shown in the figure as the outer box that has dashed lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
